# Описание подхода

## Принятые условности

1. Импорт выполняется **в рамках выбранной партии и склада**, заданных в форме. Все строки файла относятся к одному месту хранения.
2. Барабаны импортируются **только по существующим кодам** (`Drum.code`). Импорт **не создаёт** новые барабаны; если код не найден — строка помечается как некорректная.
3. Поле `position` в CSV — **уникально в пределах партии**. Правило применяются при импорте; на уровне БД гарантируется уникальность `batch + number_in_batch`.
4. Длины указываются в метрах, десятичный разделитель — точка. Валидация: `0 < length ≤ drum.initial_length_m` и глобальные пределы `0.01…1_000_000` м.
5. Номер партии (`Batch.number`) уникален глобально. Если партия с указанным номером отсутствует — создаётся.
6. Нормализация кодов и строковых полей: `.strip()` и `upper()` для кодов (склады/модели/барабаны).
7. Повторный импорт **того же файла** в ту же партию не запрещён жёстко: бизнес-логика отфильтрует дубликаты, а подробности сохранятся в `ImportLog`.

---

## Часть 1 — описание подхода

### Базовые сущности и ключи

- **Storage**: `code` (unique), `name`.
- **CableModel**: `code` (unique), `name`, `min_length_m`, `max_length_m` (+ `CheckConstraint` на корректность диапазона).
- **Drum**: `code` (unique), `initial_length_m`, `cable_model` (+ проверка, что `initial_length_m` попадает в диапазон модели).
- **Batch**: `number` (unique), timestamps.
- **BatchItem**: `batch` (FK), `drum` (FK), `storage_location` (FK), `number_in_batch` (int ≥ 1), `length_m` (Decimal).
  - **Уникальность** на уровне БД: `UniqueConstraint(batch, number_in_batch)`.
- **ImportLog**: `batch`, `file_name`, `file_sha256`, `total/inserted/duplicates_in_file/duplicates_in_db/invalid_rows`, `duration_sec`, `errors[]` (JSON).

### Исключение дублей при загрузке

1. **Дедупликация на уровне файла (по `sha256`)**  
   Перед обработкой рассчитывается `sha256` содержимого. Если в `ImportLog` уже есть запись с тем же `batch` и `file_sha256`, создаётся новый `ImportLog` с сообщением об ошибке «файл уже обработан для этой партии», и импорт прерывается исключением. Это защищает от повторного импорта **того же файла** в ту же партию.

2. **Дубли позиций внутри одного CSV**  
   Для поля `position` (положительное целое) ведётся множество `used_positions_in_file`. Если в файле встречается повтор той же `position`, строка не импортируется, счётчик `duplicates_in_file` увеличивается, а в `errors` добавляется запись вида «дублирование position … в файле».

3. **Дубли позиций относительно БД**  
   Перед вставкой собирается множество уже занятых позиций в партии: `existing_positions = {number_in_batch}` для текущего `batch`. Если `position` из файла уже присутствует в `existing_positions`, строка не вставляется и увеличивается `duplicates_in_db`.  

4. **Что не считается дублем в текущей реализации**  
   Дубли **по барабану (`drum`)** — ни в файле, ни относительно БД — **не учитываются** как отдельная категория. Валидация барабана сводится к проверке существования кода в каталоге; запрета «повторного барабана в партии» логика импорта **не содержит**.

5. **Порог качества файла**  
   После разбора файла вычисляется доля проблемных строк как  
   `(invalid_rows + duplicates_in_file) / total`.  
   Если она **> 50%**, создаётся `ImportLog` с агрегатами и сообщением «Порог >50% ошибок … — загрузка отменена», после чего импорт завершается исключением.

### Проверки данных (схема и код)

**На уровне схемы (моделей и миграций):**
- `UniqueConstraint(batch, number_in_batch)` — уникальность позиции в партии.
- `CheckConstraint(length_m__gt=0)` и `MinValueValidator/MaxValueValidator` для `length_m` (диапазон `0.01…1_000_000`).
- В `Drum` — проверка диапазона `initial_length_m` в границах `CableModel`.
- Нормализация кодов (склады/модели/барабаны) в `save()` → верхний регистр/трим.

**На уровне кода импорта:**
- Парсинг CSV с явной обработкой ошибок (невалидные типы, пустые поля, отрицательные/некорректные длины).
- Проверка наличия `Drum` по коду, сравнение `length` с `initial_length_m`.
- Отсечение дублей в файле (`drum_code`, `position`) и дублей в БД (`batch + drum`, `batch + number_in_batch`).

### Сигналы/метрики для контроля процесса

- **Audit / ImportLog**: сохраняются агрегаты `total/inserted/duplicates_in_file/duplicates_in_db/invalid_rows`, `duration_sec`, список ошибок (первые 100), `file_name`, `file_sha256`.
- В админке: вычисляемые поля статуса (`OK`/`PARTIAL`/`FAIL`), прогресс (%), фильтры по статусу. Это даёт быструю операционную видимость.
- (Опционально для продакшена) интеграция с Prometheus (`django-prometheus`) и алерты на рост `invalid_rows`/`duplicates_in_db`.

### Тестирование основных сценариев

1. **Импорт валидного файла** → создаются `Batch`, `BatchItem`’ы, `ImportLog` со статусом `OK`.
2. **Повторный импорт того же файла** → `duplicates_in_db` > 0, новых вставок нет, `ImportLog.status = PARTIAL/OK` в зависимости от сочетания.
3. **Дубликаты в файле** (`drum_code` повторяется) → `duplicates_in_file` > 0, вставляются только уникальные.
4. **Некорректные длины** (≤ 0 или > `initial_length_m`) → строки попадают в `invalid_rows`, ошибки в `errors`.
5. **Проверка ограничений БД**: попытка вставить две одинаковые `position` в рамках партии приводит к `IntegrityError` (ловится в сервисе импорта).
6. **Граничные случаи**: пустые поля, большие файлы (порог: 5MB), неверная кодировка.

### Рабочее окружение и стек

- **ОС / запуск:** Docker Compose (PostgreSQL 16 + Django 5.2), альтернативно — локально через Poetry.
- **Фреймворк:** Django 5.2 + DRF 3.16. Схема API — `drf-spectacular`.
- **СУБД:** PostgreSQL 16 (`psycopg 3.x`).
- **Структура проекта:** `src/apps/*` (модули: `core`, `catalog`, `storage`, `inventory`, `audit`), шаблон админ-страницы импорта — `templates/admin/inventory/batch/import.html`.
- **Контейнеризация:** `Dockerfile` сборка с Poetry; `docker-compose.yml` поднимает `db` и `web` (команда старта применяет миграции, собирает статику и создаёт суперпользователя из `.env`).

## Приложение: пример CSV

```csv
position,drum_code,length
1,DRUM-001,250
2,DRUM-002,300
3,DRUM-003,150.5
```

